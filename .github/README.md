# MLOps CI/CD Pipeline Architecture

This repository implements a sophisticated CI/CD pipeline system that automatically triggers different workflows based on the files that change. This path-based triggering ensures efficient resource usage and faster feedback loops.

## ğŸ—ï¸ Pipeline Architecture

### Path-Based Pipeline Triggering

Our CI/CD system uses GitHub Actions path filters to trigger specific pipelines based on which directories contain changes:

```
Repository Structure:
â”œâ”€â”€ services/         â†’ Triggers Services Pipeline
â”œâ”€â”€ data/             â†’ Triggers Data Pipeline  
â”œâ”€â”€ model/            â†’ Triggers Model Pipeline
â””â”€â”€ infrastructure/   â†’ Triggers Main Pipeline (fallback)
```

## ğŸš€ Pipeline Details

### 1. CI/CD Pipeline with Docker (`service-pipeline.yml`)

**Triggers when:** Changes in `services/**` directory

**Includes:**
- âœ… **Testing**: Python 3.11 testing with pytest
- ğŸ” **Code Quality**: Linting with flake8, coverage reporting
- ğŸ³ **Build**: Docker image building and pushing to Docker Hub
- ğŸ”— **Integration**: Comprehensive integration testing with health checks
- ğŸ”’ **Security**: Trivy vulnerability scanning with SARIF reports
- ğŸš€ **Deploy**: Automated EKS deployment via ArgoCD with GitOps

**Services Included:**
- API Service (`services/api/`) - Port 8000
- Predict Service (`services/predict/`) - Port 8001

**Advanced Features:**
- Multi-service Docker builds with proper tagging
- Container health monitoring and retry logic
- GitOps deployment with manifest updates
- ArgoCD automatic synchronization

### 2. Data Pipeline (`data-pipeline.yml`)

**Triggers when:** Changes in `data/**` directory or data-related scripts

**Includes:**
- âœ… **Data Validation**: Schema validation and quality checks
- ğŸ” **Code Quality**: Linting of data processing code
- âš™ï¸ **Data Processing**: Data preprocessing and feature engineering
- ğŸ“Š **Quality Assurance**: Data integrity and range validation
- ğŸ“¤ **Artifact Management**: Data versioning and storage
- ğŸ“¢ **Notifications**: Downstream pipeline notifications

**Current Status:** Placeholder implementation with comprehensive logging

### 3. Model Pipeline (`model-pipeline.yml`)

**Triggers when:** Changes in `model/**` directory or training scripts

**Includes:**
- âœ… **Model Validation**: Code validation and architecture checks
- ğŸ‹ï¸ **Training**: Model training with hyperparameter optimization
- ğŸ§ª **Testing**: Comprehensive model unit testing
- ğŸ“ **Registration**: Model artifact registration and versioning
- ğŸš€ **Deployment Readiness**: Pre-deployment validation
- ğŸ“¢ **Notifications**: Monitoring and downstream notifications

**Current Status:** Placeholder implementation with detailed workflow steps

### 4. Main Pipeline (`main.yml`)

**Triggers when:** Changes to infrastructure, documentation, or other files

**Includes:**
- â„¹ï¸ **Pipeline Information**: Overview of the pipeline system
- ğŸ—ï¸ **Infrastructure Checks**: Terraform and Kubernetes manifest validation
- ğŸ“š **Documentation**: Documentation completeness checks
- ğŸ” **General Linting**: YAML and root-level Python file validation

## ğŸ”§ Configuration

### Required Secrets

Add these secrets to your GitHub repository:

```bash
# Docker Hub
DOCKER_USERNAME          # Docker Hub username
DOCKER_PASSWORD          # Docker Hub password/token

# AWS Configuration  
AWS_ACCESS_KEY_ID       # AWS access key
AWS_SECRET_ACCESS_KEY   # AWS secret key
AWS_REGION              # AWS region (e.g., ap-south-1)
EKS_CLUSTER_NAME        # EKS cluster name for deployment
```

### Environment Variables

Each pipeline uses consistent environment variables:

```yaml
env:
  PYTHON_VERSION: '3.9'          # Python version for all jobs
  MLFLOW_TRACKING_URI: 'http://localhost:5000'  # MLflow tracking (model pipeline)
```

## ğŸ“ Directory Structure Impact

### Services Directory Changes
```bash
services/
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â””â”€â”€ predict/
    â”œâ”€â”€ app/
    â”œâ”€â”€ tests/
    â”œâ”€â”€ Dockerfile
    â””â”€â”€ requirements.txt
```
**Result:** Full CI/CD pipeline with testing, building, and deployment

### Data Directory Changes
```bash
data/
â”œâ”€â”€ raw/
â”œâ”€â”€ processed/
â”œâ”€â”€ features/
â””â”€â”€ src/
    â””â”€â”€ preprocess.py
```
**Result:** Data validation, quality checks, and preprocessing pipeline

### Model Directory Changes
```bash
model/
â”œâ”€â”€ src/
â”œâ”€â”€ artifacts/
â”œâ”€â”€ configs/
â””â”€â”€ *.py training scripts
```
**Result:** Model training, validation, and registration pipeline

## ğŸ¯ Pipeline Optimization Features

### 1. **Caching Strategy**
- Pip dependencies cached by directory type
- Docker layer caching for faster builds
- Separate cache keys for different pipeline types

### 2. **Parallel Execution**
- Independent jobs run in parallel within each pipeline
- Matrix builds for multi-version testing
- Concurrent security scanning

### 3. **Conditional Execution**
- Deployment only on main branch
- Different validation levels for PR vs push
- Smart path filtering to avoid unnecessary runs

### 4. **Resource Efficiency**
- Path-based triggering reduces compute usage
- Only relevant tests run for specific changes
- Targeted linting and validation

## ğŸ”„ Workflow Examples

### Example 1: API Service Update
```bash
# Change made to services/api/app/main.py
git add services/api/app/main.py
git commit -m "Update API endpoint"
git push
```
**Result:** Only `service-pipeline.yml` pipeline runs

### Example 2: Data Processing Update  
```bash
# Change made to data/src/preprocess.py
git add data/src/preprocess.py
git commit -m "Improve data preprocessing"
git push
```
**Result:** Only `data-pipeline.yml` pipeline runs

### Example 3: Model Training Update
```bash
# Change made to model/housing_models.py
git add model/housing_models.py  
git commit -m "Add new model architecture"
git push
```
**Result:** Only `model-pipeline.yml` pipeline runs

## ğŸš¨ Troubleshooting

### Pipeline Not Triggering
1. Check if file changes match the path filters
2. Verify branch is `main` or `develop` for push triggers
3. Check if files are in ignored paths

### Multiple Pipelines Triggering
- This is expected if changes span multiple directories
- Each pipeline runs independently and in parallel

### Deployment Failures
1. Verify all required secrets are configured
2. Check AWS credentials and EKS cluster access
3. Ensure Docker Hub credentials are valid

## ğŸ“ˆ Future Enhancements

### Planned Features
- [ ] MLflow integration for model tracking
- [ ] DVC integration for data versioning  
- [ ] Advanced monitoring and alerting
- [ ] Cross-pipeline dependencies
- [ ] Automatic rollback capabilities
- [ ] Performance benchmarking

### Integration Opportunities
- [ ] Slack/Teams notifications
- [ ] Jira ticket integration
- [ ] Automated PR comments with pipeline results
- [ ] Integration with monitoring dashboards

## ğŸ“Š Monitoring and Observability

Each pipeline includes comprehensive logging and status reporting:

- **Real-time Status**: GitHub Actions provides live status updates
- **Detailed Logs**: Each step includes descriptive output
- **Artifact Tracking**: Build artifacts and test results are preserved
- **Security Reports**: Vulnerability scans uploaded to Security tab
- **Coverage Reports**: Test coverage integrated with Codecov

This architecture provides a robust, scalable foundation for MLOps workflows with clear separation of concerns and efficient resource utilization.
