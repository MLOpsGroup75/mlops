name: Data Pipeline

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'data/**'
      - 'scripts/download_california_housing.py'
      - '.github/workflows/data-pipeline.yml'
  pull_request:
    branches: [ main ]
    paths:
      - 'data/**'
      - 'scripts/download_california_housing.py'
      - '.github/workflows/data-pipeline.yml'
  workflow_dispatch: # Allow manual triggering

env:
  PYTHON_VERSION: '3.9'

jobs:
  data-pipeline:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0 # Full history for DVC

    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install DVC
      run: |
        python -m pip install --upgrade pip
        pip install dvc[s3] pandas scikit-learn numpy

    - name: Configure Git for DVC
      run: |
        git config --global user.name "GitHub Actions"
        git config --global user.email "actions@github.com"

    - name: Setup DVC
      run: |
        dvc remote add -d s3remote s3://mlops-housing-dev-datasets
        # dvc config core.remote s3remote
        # dvc config core.remote.s3remote.url s3://mlops-housing-dev-datasets
        # dvc config core.remote.s3remote.region us-east-1

    - name: Configure AWS credentials
      run: |
        mkdir -p ~/.aws
        echo "[default]" > ~/.aws/credentials
        echo "aws_access_key_id = ${{ secrets.AWS_ACCESS_KEY_ID }}" >> ~/.aws/credentials
        echo "aws_secret_access_key = ${{ secrets.AWS_SECRET_ACCESS_KEY }}" >> ~/.aws/credentials
        echo "[default]" > ~/.aws/config
        echo "region = us-east-1" >> ~/.aws/config

    - name: Pull existing data from DVC
      run: |
        echo "üì• Pulling existing data from DVC remote..."
        dvc pull --all-branches || echo "No existing data to pull"

    - name: Run data processing
      run: |
        echo "‚öôÔ∏è Running data processing pipeline..."
        cd data/src
        python preprocess.py
        echo "‚úÖ Data processing completed!"

    - name: Add and version datasets with DVC
      run: |
        echo "üìä Versioning datasets with DVC..."

        # Add raw data
        if [ -d "data/raw" ]; then
          cd data/raw
          for file in *.csv; do
            if [ -f "$file" ]; then
              echo "Adding $file to DVC..."
              dvc add "$file"
            fi
          done
          cd ../..
        fi

        # Add processed data
        if [ -d "data/processed" ]; then
          cd data/processed
          for file in *.csv; do
            if [ -f "$file" ]; then
              echo "Adding $file to DVC..."
              dvc add "$file"
            fi
          done
          cd ../..
        fi

        echo "‚úÖ All datasets versioned with DVC!"

    - name: Push data to DVC remote
      run: |
        echo "üì§ Pushing data to DVC remote..."
        dvc push --all-branches
        echo "‚úÖ Data pushed to DVC remote!"

    - name: Commit and push .dvc files
      run: |
        echo "üîÄ Committing and pushing .dvc files..."

        # Add all .dvc files
        git add data/**/*.dvc

        # Check if there are changes to commit
        if git diff --staged --quiet; then
          echo "No changes in .dvc files to commit"
        else
          # Commit .dvc files
          git commit -m "Update dataset versions [skip ci]"

          # Push to current branch
          git push origin ${{ github.ref }}
          echo "‚úÖ .dvc files committed and pushed!"
        fi

    - name: Generate data summary
      run: |
        echo "üìã Generating data summary..."
        python -c "
        import pandas as pd
        import json
        import os

        summary = {}
        for root, dirs, files in os.walk('data'):
            for file in files:
                if file.endswith('.csv'):
                    filepath = os.path.join(root, file)
                    try:
                        df = pd.read_csv(filepath)
                        summary[filepath] = {
                            'rows': len(df),
                            'columns': len(df.columns),
                            'size_mb': round(os.path.getsize(filepath) / (1024*1024), 2)
                        }
                    except Exception as e:
                        summary[filepath] = {'error': str(e)}

        with open('artifacts/data_summary.json', 'w') as f:
            json.dump(summary, f, indent=2)

        print('Data summary generated:')
        for filepath, info in summary.items():
            if 'error' not in info:
                print(f'{filepath}: {info[\"rows\"]} rows, {info[\"columns\"]} columns, {info[\"size_mb\"]} MB')
        "

    - name: Upload data summary artifact
      uses: actions/upload-artifact@v4
      with:
        name: data-summary
        path: artifacts/data_summary.json

    - name: Pipeline completion
      run: |
        echo "üéâ Data Pipeline Completed Successfully!"
        echo "======================================"
        echo "‚úÖ Data processed and versioned"
        echo "‚úÖ Data pushed to DVC remote"
        echo "‚úÖ .dvc files committed to git"
        echo "‚úÖ Data summary generated"
        echo "======================================"
