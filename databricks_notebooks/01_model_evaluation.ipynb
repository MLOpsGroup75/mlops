{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb1d6fb9-61e2-4be7-b0d2-7a8cd522b1c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Model Evaluation Task\n",
    "\n",
    "This notebook defines the evaluation task for the California Housing MLOps pipeline.\n",
    "It uses standard MLflow methods to produce comprehensive validation metrics for model versions.\n",
    "\n",
    "## Features:\n",
    "- Automated model evaluation using standard MLflow metrics\n",
    "- Custom metrics and visualizations\n",
    "- Evaluation results logging\n",
    "- Model performance comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27d9ed18-dc33-4a80-939c-358e1692f32a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9b36006-d61e-45a6-aa39-b12fef37bd15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: typing_extensions in /local_disk0/.ephemeral_nfs/envs/pythonEnv-296502da-18ae-4c78-a178-7a30bee712c2/lib/python3.11/site-packages (4.14.1)\nRequirement already satisfied: mlflow in /local_disk0/.ephemeral_nfs/envs/pythonEnv-296502da-18ae-4c78-a178-7a30bee712c2/lib/python3.11/site-packages (3.2.0)\nRequirement already satisfied: mlflow-skinny==3.2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-296502da-18ae-4c78-a178-7a30bee712c2/lib/python3.11/site-packages (from mlflow) (3.2.0)\nRequirement already satisfied: mlflow-tracing==3.2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-296502da-18ae-4c78-a178-7a30bee712c2/lib/python3.11/site-packages (from mlflow) (3.2.0)\nRequirement already satisfied: Flask<4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-296502da-18ae-4c78-a178-7a30bee712c2/lib/python3.11/site-packages (from mlflow) (3.1.1)\nRequirement already satisfied: alembic!=1.10.0,<2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-296502da-18ae-4c78-a178-7a30bee712c2/lib/python3.11/site-packages (from mlflow) (1.16.4)\nRequirement already satisfied: docker<8,>=4.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-296502da-18ae-4c78-a178-7a30bee712c2/lib/python3.11/site-packages (from mlflow) (7.1.0)\nRequirement already satisfied: graphene<4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-296502da-18ae-4c78-a178-7a30bee712c2/lib/python3.11/site-packages (from mlflow) (3.4.3)\nRequirement already satisfied: gunicorn<24 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-296502da-18ae-4c78-a178-7a30bee712c2/lib/python3.11/site-packages (from mlflow) (23.0.0)\nRequirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.11/site-packages (from mlflow) (3.7.2)\nRequirement already satisfied: numpy<3 in /databricks/python3/lib/python3.11/site-packages (from mlflow) (1.23.5)\nRequirement already satisfied: pandas<3 in /databricks/python3/lib/python3.11/site-packages (from mlflow) (1.5.3)\nRequirement already satisfied: pyarrow<22,>=4.0.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow) (14.0.1)\nRequirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.11/site-packages (from mlflow) (1.3.0)\nRequirement already satisfied: scipy<2 in /databricks/python3/lib/python3.11/site-packages (from mlflow) (1.11.1)\nRequirement already satisfied: sqlalchemy<3,>=1.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-296502da-18ae-4c78-a178-7a30bee712c2/lib/python3.11/site-packages (from mlflow) (2.0.42)\nRequirement already satisfied: cachetools<7,>=5.0.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny==3.2.0->mlflow) (5.5.0)\nRequirement already satisfied: click<9,>=7.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-296502da-18ae-4c78-a178-7a30bee712c2/lib/python3.11/site-packages (from mlflow-skinny==3.2.0->mlflow) (8.2.1)\nRequirement already satisfied: cloudpickle<4 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny==3.2.0->mlflow) (3.0.0)\nRequirement already satisfied: databricks-sdk<1,>=0.20.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny==3.2.0->mlflow) (0.40.0)\nRequirement already satisfied: fastapi<1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-296502da-18ae-4c78-a178-7a30bee712c2/lib/python3.11/site-packages (from mlflow-skinny==3.2.0->mlflow) (0.116.1)\nRequirement already satisfied: gitpython<4,>=3.1.9 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny==3.2.0->mlflow) (3.1.43)\nRequirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny==3.2.0->mlflow) (6.0.0)\nRequirement already satisfied: opentelemetry-api<3,>=1.9.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-296502da-18ae-4c78-a178-7a30bee712c2/lib/python3.11/site-packages (from mlflow-skinny==3.2.0->mlflow) (1.36.0)\nRequirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-296502da-18ae-4c78-a178-7a30bee712c2/lib/python3.11/site-packages (from mlflow-skinny==3.2.0->mlflow) (1.36.0)\nRequirement already satisfied: packaging<26 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny==3.2.0->mlflow) (23.2)\nRequirement already satisfied: protobuf<7,>=3.12.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny==3.2.0->mlflow) (5.29.3)\nRequirement already satisfied: pydantic<3,>=1.10.8 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-296502da-18ae-4c78-a178-7a30bee712c2/lib/python3.11/site-packages (from mlflow-skinny==3.2.0->mlflow) (2.11.7)\nRequirement already satisfied: pyyaml<7,>=5.1 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny==3.2.0->mlflow) (6.0)\nRequirement already satisfied: requests<3,>=2.17.3 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny==3.2.0->mlflow) (2.31.0)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny==3.2.0->mlflow) (0.5.1)\nRequirement already satisfied: uvicorn<1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-296502da-18ae-4c78-a178-7a30bee712c2/lib/python3.11/site-packages (from mlflow-skinny==3.2.0->mlflow) (0.35.0)\nRequirement already satisfied: Mako in /local_disk0/.ephemeral_nfs/envs/pythonEnv-296502da-18ae-4c78-a178-7a30bee712c2/lib/python3.11/site-packages (from alembic!=1.10.0,<2->mlflow) (1.3.10)\nRequirement already satisfied: urllib3>=1.26.0 in /databricks/python3/lib/python3.11/site-packages (from docker<8,>=4.0.0->mlflow) (1.26.16)\nRequirement already satisfied: blinker>=1.9.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-296502da-18ae-4c78-a178-7a30bee712c2/lib/python3.11/site-packages (from Flask<4->mlflow) (1.9.0)\nRequirement already satisfied: itsdangerous>=2.2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-296502da-18ae-4c78-a178-7a30bee712c2/lib/python3.11/site-packages (from Flask<4->mlflow) (2.2.0)\nRequirement already satisfied: jinja2>=3.1.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-296502da-18ae-4c78-a178-7a30bee712c2/lib/python3.11/site-packages (from Flask<4->mlflow) (3.1.6)\nRequirement already satisfied: markupsafe>=2.1.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-296502da-18ae-4c78-a178-7a30bee712c2/lib/python3.11/site-packages (from Flask<4->mlflow) (3.0.2)\nRequirement already satisfied: werkzeug>=3.1.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-296502da-18ae-4c78-a178-7a30bee712c2/lib/python3.11/site-packages (from Flask<4->mlflow) (3.1.3)\nRequirement already satisfied: graphql-core<3.3,>=3.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-296502da-18ae-4c78-a178-7a30bee712c2/lib/python3.11/site-packages (from graphene<4->mlflow) (3.2.6)\nRequirement already satisfied: graphql-relay<3.3,>=3.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-296502da-18ae-4c78-a178-7a30bee712c2/lib/python3.11/site-packages (from graphene<4->mlflow) (3.2.0)\nRequirement already satisfied: python-dateutil<3,>=2.7.0 in /databricks/python3/lib/python3.11/site-packages (from graphene<4->mlflow) (2.8.2)\nRequirement already satisfied: contourpy>=1.0.1 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow) (1.0.5)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow) (4.25.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow) (1.4.4)\nRequirement already satisfied: pillow>=6.2.0 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow) (10.3.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /databricks/python3/lib/python3.11/site-packages (from matplotlib<4->mlflow) (3.0.9)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.11/site-packages (from pandas<3->mlflow) (2022.7)\nRequirement already satisfied: joblib>=1.1.1 in /databricks/python3/lib/python3.11/site-packages (from scikit-learn<2->mlflow) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.11/site-packages (from scikit-learn<2->mlflow) (2.2.0)\nRequirement already satisfied: greenlet>=1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-296502da-18ae-4c78-a178-7a30bee712c2/lib/python3.11/site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.2.4)\nRequirement already satisfied: google-auth~=2.0 in /databricks/python3/lib/python3.11/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.2.0->mlflow) (2.35.0)\nRequirement already satisfied: starlette<0.48.0,>=0.40.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-296502da-18ae-4c78-a178-7a30bee712c2/lib/python3.11/site-packages (from fastapi<1->mlflow-skinny==3.2.0->mlflow) (0.47.2)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.11/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.2.0->mlflow) (4.0.11)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.11/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.2.0->mlflow) (3.11.0)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-296502da-18ae-4c78-a178-7a30bee712c2/lib/python3.11/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.2.0->mlflow) (0.57b0)\nRequirement already satisfied: annotated-types>=0.6.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-296502da-18ae-4c78-a178-7a30bee712c2/lib/python3.11/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.2.0->mlflow) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-296502da-18ae-4c78-a178-7a30bee712c2/lib/python3.11/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.2.0->mlflow) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-296502da-18ae-4c78-a178-7a30bee712c2/lib/python3.11/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.2.0->mlflow) (0.4.1)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.2.0->mlflow) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.2.0->mlflow) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.11/site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.2.0->mlflow) (2023.7.22)\nRequirement already satisfied: h11>=0.8 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-296502da-18ae-4c78-a178-7a30bee712c2/lib/python3.11/site-packages (from uvicorn<1->mlflow-skinny==3.2.0->mlflow) (0.16.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.2.0->mlflow) (5.0.1)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.2.0->mlflow) (0.2.8)\nRequirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.2.0->mlflow) (4.9)\nRequirement already satisfied: anyio<5,>=3.6.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-296502da-18ae-4c78-a178-7a30bee712c2/lib/python3.11/site-packages (from starlette<0.48.0,>=0.40.0->fastapi<1->mlflow-skinny==3.2.0->mlflow) (4.10.0)\nRequirement already satisfied: sniffio>=1.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-296502da-18ae-4c78-a178-7a30bee712c2/lib/python3.11/site-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi<1->mlflow-skinny==3.2.0->mlflow) (1.3.1)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.2.0->mlflow) (0.4.8)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: scikit-learn==1.3.0 in /databricks/python3/lib/python3.11/site-packages (1.3.0)\nRequirement already satisfied: numpy>=1.17.3 in /databricks/python3/lib/python3.11/site-packages (from scikit-learn==1.3.0) (1.23.5)\nRequirement already satisfied: scipy>=1.5.0 in /databricks/python3/lib/python3.11/site-packages (from scikit-learn==1.3.0) (1.11.1)\nRequirement already satisfied: joblib>=1.1.1 in /databricks/python3/lib/python3.11/site-packages (from scikit-learn==1.3.0) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.11/site-packages (from scikit-learn==1.3.0) (2.2.0)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade typing_extensions mlflow\n",
    "\n",
    "# Import required libraries\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "# Note: mlflow.evaluate requires MLflow 2.0+\n",
    "# Using standard MLflow evaluation for compatibility\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mlflow.tracking import MlflowClient\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from typing import Dict, Any, List, Optional\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e1a4164-cb6d-45e3-8f73-80d45907001c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b4cd04f-2dca-44d4-8ab3-83fc633e3698",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: workspace.default.california_housing_predictor\nModel Version: 2\nEvaluation Experiment: /Shared/mlops/model_evaluation\nEvaluation Type: comprehensive\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "dbutils.widgets.text(\"model_name\", \"california_housing_predictor\", \"Registered Model Name\")\n",
    "dbutils.widgets.text(\"model_version\", \"latest\", \"Model Version (or 'latest')\")\n",
    "dbutils.widgets.text(\"evaluation_experiment\", \"/Shared/mlops/model_evaluation\", \"Evaluation Experiment Path\")\n",
    "dbutils.widgets.text(\"data_path\", \"/databricks-datasets/learning-spark-v2/sf-airbnb/sf-airbnb-clean.parquet\", \"Test Data Path\")\n",
    "dbutils.widgets.dropdown(\"evaluation_type\", \"comprehensive\", [\"quick\", \"comprehensive\"], \"Evaluation Type\")\n",
    "\n",
    "# Get parameters\n",
    "MODEL_NAME = dbutils.widgets.get(\"model_name\")\n",
    "MODEL_VERSION = dbutils.widgets.get(\"model_version\") \n",
    "EVALUATION_EXPERIMENT = dbutils.widgets.get(\"evaluation_experiment\")\n",
    "DATA_PATH = dbutils.widgets.get(\"data_path\")\n",
    "EVALUATION_TYPE = dbutils.widgets.get(\"evaluation_type\")\n",
    "\n",
    "print(f\"Model Name: {MODEL_NAME}\")\n",
    "print(f\"Model Version: {MODEL_VERSION}\")\n",
    "print(f\"Evaluation Experiment: {EVALUATION_EXPERIMENT}\")\n",
    "print(f\"Evaluation Type: {EVALUATION_TYPE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "924e39ad-b991-4efc-a6ff-1b5cc2c467d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "434fcb7a-d28c-4adc-99e5-276c82caa4cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-10 08:18:44,351 24812 INFO to_pandas Executing plan <Truncated message due to truncation error>\n2025-08-10 08:18:44,351 24812 INFO to_pandas Executing plan <Truncated message due to truncation error>\nINFO:pyspark.sql.connect.client.logging:Executing plan <Truncated message due to truncation error>\n2025-08-10 08:18:44,787 24812 INFO _execute_and_fetch ExecuteAndFetch\n2025-08-10 08:18:44,787 24812 INFO _execute_and_fetch ExecuteAndFetch\nINFO:pyspark.sql.connect.client.logging:ExecuteAndFetch\n2025-08-10 08:18:44,790 24812 INFO _execute_and_fetch_as_iterator ExecuteAndFetchAsIterator\n2025-08-10 08:18:44,790 24812 INFO _execute_and_fetch_as_iterator ExecuteAndFetchAsIterator\nINFO:pyspark.sql.connect.client.logging:ExecuteAndFetchAsIterator\n2025-08-10 08:18:45,052 24812 ERROR _handle_rpc_error GRPC Error received\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py\", line 1723, in _execute_and_fetch_as_iterator\n    for b in generator:\n  File \"<frozen _collections_abc>\", line 330, in __next__\n  File \"/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py\", line 139, in send\n    if not self._has_next():\n           ^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py\", line 200, in _has_next\n    raise e\n  File \"/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py\", line 172, in _has_next\n    self._current = self._call_iter(\n                    ^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py\", line 297, in _call_iter\n    raise e\n  File \"/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py\", line 277, in _call_iter\n    return iter_fun()\n           ^^^^^^^^^^\n  File \"/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py\", line 173, in <lambda>\n    lambda: next(self._iterator)  # type: ignore[arg-type]\n            ^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.11/site-packages/grpc/_channel.py\", line 543, in __next__\n    return self._next()\n           ^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.11/site-packages/grpc/_channel.py\", line 969, in _next\n    raise self\ngrpc._channel._MultiThreadedRendezvous: <_MultiThreadedRendezvous of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"[PATH_NOT_FOUND] Path does not exist: dbfs:/databricks-datasets/california-housing/cal_housing.data. SQLSTATE: 42K03\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"[PATH_NOT_FOUND] Path does not exist: dbfs:/databricks-datasets/california-housing/cal_housing.data. SQLSTATE: 42K03\", grpc_status:13, created_time:\"2025-08-10T08:18:45.05222633+00:00\"}\"\n>\n2025-08-10 08:18:45,052 24812 ERROR _handle_rpc_error GRPC Error received\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py\", line 1723, in _execute_and_fetch_as_iterator\n    for b in generator:\n  File \"<frozen _collections_abc>\", line 330, in __next__\n  File \"/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py\", line 139, in send\n    if not self._has_next():\n           ^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py\", line 200, in _has_next\n    raise e\n  File \"/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py\", line 172, in _has_next\n    self._current = self._call_iter(\n                    ^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py\", line 297, in _call_iter\n    raise e\n  File \"/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py\", line 277, in _call_iter\n    return iter_fun()\n           ^^^^^^^^^^\n  File \"/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py\", line 173, in <lambda>\n    lambda: next(self._iterator)  # type: ignore[arg-type]\n            ^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.11/site-packages/grpc/_channel.py\", line 543, in __next__\n    return self._next()\n           ^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.11/site-packages/grpc/_channel.py\", line 969, in _next\n    raise self\ngrpc._channel._MultiThreadedRendezvous: <_MultiThreadedRendezvous of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"[PATH_NOT_FOUND] Path does not exist: dbfs:/databricks-datasets/california-housing/cal_housing.data. SQLSTATE: 42K03\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"[PATH_NOT_FOUND] Path does not exist: dbfs:/databricks-datasets/california-housing/cal_housing.data. SQLSTATE: 42K03\", grpc_status:13, created_time:\"2025-08-10T08:18:45.05222633+00:00\"}\"\n>\nERROR:pyspark.sql.connect.client.logging:GRPC Error received\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py\", line 1723, in _execute_and_fetch_as_iterator\n    for b in generator:\n  File \"<frozen _collections_abc>\", line 330, in __next__\n  File \"/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py\", line 139, in send\n    if not self._has_next():\n           ^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py\", line 200, in _has_next\n    raise e\n  File \"/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py\", line 172, in _has_next\n    self._current = self._call_iter(\n                    ^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py\", line 297, in _call_iter\n    raise e\n  File \"/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py\", line 277, in _call_iter\n    return iter_fun()\n           ^^^^^^^^^^\n  File \"/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py\", line 173, in <lambda>\n    lambda: next(self._iterator)  # type: ignore[arg-type]\n            ^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.11/site-packages/grpc/_channel.py\", line 543, in __next__\n    return self._next()\n           ^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.11/site-packages/grpc/_channel.py\", line 969, in _next\n    raise self\ngrpc._channel._MultiThreadedRendezvous: <_MultiThreadedRendezvous of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"[PATH_NOT_FOUND] Path does not exist: dbfs:/databricks-datasets/california-housing/cal_housing.data. SQLSTATE: 42K03\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"[PATH_NOT_FOUND] Path does not exist: dbfs:/databricks-datasets/california-housing/cal_housing.data. SQLSTATE: 42K03\", grpc_status:13, created_time:\"2025-08-10T08:18:45.05222633+00:00\"}\"\n>\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load from DBFS datasets: [PATH_NOT_FOUND] Path does not exist: dbfs:/databricks-datasets/california-housing/cal_housing.data. SQLSTATE: 42K03\n\nJVM stacktrace:\norg.apache.spark.sql.AnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(QueryCompilationErrors.scala:2419)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:1068)\n\tat scala.collection.immutable.List.flatMap(List.scala:294)\n\tat scala.collection.immutable.List.flatMap(List.scala:79)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:1049)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:677)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:500)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:234)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:97)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:97)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:58)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:58)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:56)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:485)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:639)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:623)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:485)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:331)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:483)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:482)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:478)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:595)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:595)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:595)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:500)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:493)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:493)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:427)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:480)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:480)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:300)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:615)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:698)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:154)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:61)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:135)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:698)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1339)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:691)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:688)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:688)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:294)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:293)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:332)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:273)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:129)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1157)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1157)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:127)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:170)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:148)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformReadRel(SparkConnectPlanner.scala:1845)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.$anonfun$transformRelation$1(SparkConnectPlanner.scala:208)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$7(SessionHolder.scala:616)\n\tat org.apache.spark.sql.connect.service.SessionHolder.measureSubtreeRelationNodes(SessionHolder.scala:632)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$5(SessionHolder.scala:615)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.connect.service.SessionHolder.usePlanCache(SessionHolder.scala:613)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:639)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$14(SqlGatewayHistorySparkListener.scala:685)\n\tat scala.Option.map(Option.scala:242)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$13(SqlGatewayHistorySparkListener.scala:685)\n\tat scala.Option.foreach(Option.scala:437)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:682)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:624)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:231)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:219)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat com.databricks.spark.sqlgateway.history.utils.RuntimeVersionBridge$$anonfun$onOtherEvent$1.applyOrElse(RuntimeVersionBridge.scala:209)\n\tat com.databricks.spark.sqlgateway.history.utils.RuntimeVersionBridge$$anonfun$onOtherEvent$1.applyOrElse(RuntimeVersionBridge.scala:209)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:197)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:197)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:42)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:42)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:199)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:169)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:116)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:116)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:111)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:107)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1575)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:107)\nGenerating synthetic test data...\nGenerated synthetic test data: 1000 samples, 8 features\n\nTest Data Overview:\nFeatures shape: (1000, 8)\nTarget shape: (1000,)\nFeatures: ['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income']\nTarget range: 4837.83 - 926720.60\n"
     ]
    }
   ],
   "source": [
    "def load_test_data():\n",
    "    \"\"\"Load test data for evaluation.\"\"\"\n",
    "    try:\n",
    "        # For California Housing dataset, try to load from DBFS or local source\n",
    "        # This is a placeholder - adapt based on your actual data location\n",
    "        \n",
    "        # Option 1: Load from DBFS if uploaded\n",
    "        try:\n",
    "            df = spark.read.csv(\"/databricks-datasets/california-housing/cal_housing.data\", \n",
    "                              header=False, inferSchema=True)\n",
    "            # California Housing column names\n",
    "            columns = ['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n",
    "                      'total_bedrooms', 'population', 'households', 'median_income', 'median_house_value']\n",
    "            \n",
    "            for i, col_name in enumerate(columns):\n",
    "                df = df.withColumnRenamed(f\"_c{i}\", col_name)\n",
    "            \n",
    "            # Convert to Pandas for sklearn compatibility\n",
    "            df_pandas = df.toPandas()\n",
    "            \n",
    "            # Prepare features and target\n",
    "            X = df_pandas.drop('median_house_value', axis=1)\n",
    "            y = df_pandas['median_house_value']\n",
    "            \n",
    "            print(f\"Loaded test data: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "            return X, y\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Could not load from DBFS datasets: {e}\")\n",
    "            \n",
    "            # Option 2: Generate synthetic test data for demonstration\n",
    "            print(\"Generating synthetic test data...\")\n",
    "            np.random.seed(42)\n",
    "            n_samples = 1000\n",
    "            \n",
    "            X = pd.DataFrame({\n",
    "                'longitude': np.random.uniform(-124, -114, n_samples),\n",
    "                'latitude': np.random.uniform(32, 42, n_samples),\n",
    "                'housing_median_age': np.random.uniform(1, 52, n_samples),\n",
    "                'total_rooms': np.random.uniform(1, 10000, n_samples),\n",
    "                'total_bedrooms': np.random.uniform(1, 2000, n_samples),\n",
    "                'population': np.random.uniform(1, 8000, n_samples),\n",
    "                'households': np.random.uniform(1, 2000, n_samples),\n",
    "                'median_income': np.random.uniform(0.5, 15, n_samples)\n",
    "            })\n",
    "            \n",
    "            # Generate target with some relationship to features\n",
    "            y = (X['median_income'] * 50000 + \n",
    "                 X['total_rooms'] * 10 + \n",
    "                 np.random.normal(0, 50000, n_samples))\n",
    "            y = np.abs(y)  # Ensure positive prices\n",
    "            \n",
    "            print(f\"Generated synthetic test data: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "            return X, y\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading test data: {e}\")\n",
    "        raise\n",
    "\n",
    "# Load test data\n",
    "X_test, y_test = load_test_data()\n",
    "\n",
    "# Display data info\n",
    "print(\"\\nTest Data Overview:\")\n",
    "print(f\"Features shape: {X_test.shape}\")\n",
    "print(f\"Target shape: {y_test.shape}\")\n",
    "print(f\"Features: {list(X_test.columns)}\")\n",
    "print(f\"Target range: {y_test.min():.2f} - {y_test.max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef996a5b-e44b-4080-8f5b-669ae8a43038",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Model Loading and Basic Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eab52b24-80c2-4cbb-866f-b94a9fcdf2d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow version: 3.2.0\nScikit-learn version: 1.3.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error loading model: No module named '_loss'\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5738834622522677>, line 48\u001B[0m\n",
       "\u001B[1;32m     45\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;66;03m# Load model\u001B[39;00m\n",
       "\u001B[0;32m---> 48\u001B[0m model, model_version_info \u001B[38;5;241m=\u001B[39m load_model_for_evaluation(MODEL_NAME, MODEL_VERSION)\n",
       "\n",
       "File \u001B[0;32m<command-5738834622522677>, line 32\u001B[0m, in \u001B[0;36mload_model_for_evaluation\u001B[0;34m(model_name, version)\u001B[0m\n",
       "\u001B[1;32m     30\u001B[0m \u001B[38;5;66;03m# Load model\u001B[39;00m\n",
       "\u001B[1;32m     31\u001B[0m model_uri \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodels:/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mversion\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m---> 32\u001B[0m model \u001B[38;5;241m=\u001B[39m mlflow\u001B[38;5;241m.\u001B[39msklearn\u001B[38;5;241m.\u001B[39mload_model(model_uri)\n",
       "\u001B[1;32m     34\u001B[0m \u001B[38;5;66;03m# Get model version info\u001B[39;00m\n",
       "\u001B[1;32m     35\u001B[0m model_version \u001B[38;5;241m=\u001B[39m client\u001B[38;5;241m.\u001B[39mget_model_version(model_name, version)\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-296502da-18ae-4c78-a178-7a30bee712c2/lib/python3.11/site-packages/mlflow/sklearn/__init__.py:657\u001B[0m, in \u001B[0;36mload_model\u001B[0;34m(model_uri, dst_path)\u001B[0m\n",
       "\u001B[1;32m    655\u001B[0m sklearn_model_artifacts_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(local_model_path, flavor_conf[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpickled_model\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
       "\u001B[1;32m    656\u001B[0m serialization_format \u001B[38;5;241m=\u001B[39m flavor_conf\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mserialization_format\u001B[39m\u001B[38;5;124m\"\u001B[39m, SERIALIZATION_FORMAT_PICKLE)\n",
       "\u001B[0;32m--> 657\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _load_model_from_local_file(\n",
       "\u001B[1;32m    658\u001B[0m     path\u001B[38;5;241m=\u001B[39msklearn_model_artifacts_path, serialization_format\u001B[38;5;241m=\u001B[39mserialization_format\n",
       "\u001B[1;32m    659\u001B[0m )\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-296502da-18ae-4c78-a178-7a30bee712c2/lib/python3.11/site-packages/mlflow/sklearn/__init__.py:476\u001B[0m, in \u001B[0;36m_load_model_from_local_file\u001B[0;34m(path, serialization_format)\u001B[0m\n",
       "\u001B[1;32m    473\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m serialization_format \u001B[38;5;241m==\u001B[39m SERIALIZATION_FORMAT_CLOUDPICKLE:\n",
       "\u001B[1;32m    474\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcloudpickle\u001B[39;00m\n",
       "\u001B[0;32m--> 476\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cloudpickle\u001B[38;5;241m.\u001B[39mload(f)\n",
       "\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named '_loss'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ModuleNotFoundError",
        "evalue": "No module named '_loss'"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>ModuleNotFoundError</span>: No module named '_loss'"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
        "File \u001B[0;32m<command-5738834622522677>, line 48\u001B[0m\n\u001B[1;32m     45\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;66;03m# Load model\u001B[39;00m\n\u001B[0;32m---> 48\u001B[0m model, model_version_info \u001B[38;5;241m=\u001B[39m load_model_for_evaluation(MODEL_NAME, MODEL_VERSION)\n",
        "File \u001B[0;32m<command-5738834622522677>, line 32\u001B[0m, in \u001B[0;36mload_model_for_evaluation\u001B[0;34m(model_name, version)\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;66;03m# Load model\u001B[39;00m\n\u001B[1;32m     31\u001B[0m model_uri \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodels:/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mversion\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 32\u001B[0m model \u001B[38;5;241m=\u001B[39m mlflow\u001B[38;5;241m.\u001B[39msklearn\u001B[38;5;241m.\u001B[39mload_model(model_uri)\n\u001B[1;32m     34\u001B[0m \u001B[38;5;66;03m# Get model version info\u001B[39;00m\n\u001B[1;32m     35\u001B[0m model_version \u001B[38;5;241m=\u001B[39m client\u001B[38;5;241m.\u001B[39mget_model_version(model_name, version)\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-296502da-18ae-4c78-a178-7a30bee712c2/lib/python3.11/site-packages/mlflow/sklearn/__init__.py:657\u001B[0m, in \u001B[0;36mload_model\u001B[0;34m(model_uri, dst_path)\u001B[0m\n\u001B[1;32m    655\u001B[0m sklearn_model_artifacts_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(local_model_path, flavor_conf[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpickled_model\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m    656\u001B[0m serialization_format \u001B[38;5;241m=\u001B[39m flavor_conf\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mserialization_format\u001B[39m\u001B[38;5;124m\"\u001B[39m, SERIALIZATION_FORMAT_PICKLE)\n\u001B[0;32m--> 657\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _load_model_from_local_file(\n\u001B[1;32m    658\u001B[0m     path\u001B[38;5;241m=\u001B[39msklearn_model_artifacts_path, serialization_format\u001B[38;5;241m=\u001B[39mserialization_format\n\u001B[1;32m    659\u001B[0m )\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-296502da-18ae-4c78-a178-7a30bee712c2/lib/python3.11/site-packages/mlflow/sklearn/__init__.py:476\u001B[0m, in \u001B[0;36m_load_model_from_local_file\u001B[0;34m(path, serialization_format)\u001B[0m\n\u001B[1;32m    473\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m serialization_format \u001B[38;5;241m==\u001B[39m SERIALIZATION_FORMAT_CLOUDPICKLE:\n\u001B[1;32m    474\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcloudpickle\u001B[39;00m\n\u001B[0;32m--> 476\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cloudpickle\u001B[38;5;241m.\u001B[39mload(f)\n",
        "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named '_loss'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"MLflow version: {mlflow.__version__}\")\n",
    "import sklearn\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
    "\n",
    "\n",
    "from typing import Any\n",
    "def load_model_for_evaluation(model_name: str, version: str) -> Any:\n",
    "\n",
    "    \"\"\"Load model from MLflow Model Registry.\"\"\"\n",
    "    try:\n",
    "        from mlflow.tracking import MlflowClient\n",
    "        import mlflow.sklearn\n",
    "        import logging\n",
    "\n",
    "        logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Get model version\n",
    "        if version.lower() == \"latest\":\n",
    "            # Get latest version\n",
    "            client = MlflowClient()\n",
    "            latest_versions = client.get_latest_versions(model_name, stages=[\"None\", \"Staging\", \"Production\"])\n",
    "            if not latest_versions:\n",
    "                raise ValueError(f\"No versions found for model {model_name}\")\n",
    "            \n",
    "            # Sort by version number and get the latest\n",
    "            latest_version = max(latest_versions, key=lambda x: int(x.version))\n",
    "            version = latest_version.version\n",
    "            print(f\"Using latest version: {version}\")\n",
    "        \n",
    "        # Load model\n",
    "        model_uri = f\"models:/{model_name}/{version}\"\n",
    "        model = mlflow.sklearn.load_model(model_uri)\n",
    "        \n",
    "        # Get model version info\n",
    "        model_version = client.get_model_version(model_name, version)\n",
    "        \n",
    "        print(f\"Loaded model: {model_name} version {version}\")\n",
    "        print(f\"Model stage: {model_version.current_stage}\")\n",
    "        print(f\"Model description: {model_version.description}\")\n",
    "        \n",
    "        return model, model_version\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading model: {e}\")\n",
    "        raise\n",
    "\n",
    "# Load model\n",
    "model, model_version_info = load_model_for_evaluation(MODEL_NAME, MODEL_VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76e898c7-2f06-47e1-96ce-539568a816b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Custom Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "699253b5-0f99-4017-b3b0-6ba726cd5916",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {
        "data_path": "/databricks-datasets/learning-spark-v2/sf-airbnb/sf-airbnb-clean.parquet",
        "evaluation_experiment": "/Shared/mlops/model_evaluation",
        "evaluation_type": "comprehensive",
        "model_name": "workspace.default.california_housing_predictor",
        "model_version": "2"
       },
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def custom_metrics():\n",
    "    \"\"\"Define custom metrics for evaluation (kept for reference).\"\"\"\n",
    "    \n",
    "    def mean_absolute_percentage_error(y_true, y_pred):\n",
    "        \"\"\"Calculate Mean Absolute Percentage Error.\"\"\"\n",
    "        return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    \n",
    "    def root_mean_squared_error(y_true, y_pred):\n",
    "        \"\"\"Calculate Root Mean Squared Error.\"\"\"\n",
    "        return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    \n",
    "    def explained_variance_score(y_true, y_pred):\n",
    "        \"\"\"Calculate Explained Variance Score.\"\"\"\n",
    "        from sklearn.metrics import explained_variance_score as evs\n",
    "        return evs(y_true, y_pred)\n",
    "    \n",
    "    def prediction_bounds_coverage(y_true, y_pred):\n",
    "        \"\"\"Calculate percentage of predictions within reasonable bounds.\"\"\"\n",
    "        # Define reasonable bounds for California housing prices\n",
    "        min_price, max_price = 50000, 2000000\n",
    "        within_bounds = np.sum((y_pred >= min_price) & (y_pred <= max_price))\n",
    "        return (within_bounds / len(y_pred)) * 100\n",
    "    \n",
    "    return {\n",
    "        \"mape\": mean_absolute_percentage_error,\n",
    "        \"rmse\": root_mean_squared_error,\n",
    "        \"explained_variance\": explained_variance_score,\n",
    "        \"bounds_coverage\": prediction_bounds_coverage\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29202a7d-56a5-4d1e-9b74-182c529ed26c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. MLflow Evaluation Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c55fa244-175f-4791-8d96-b8897a9f08f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {
        "data_path": "/databricks-datasets/learning-spark-v2/sf-airbnb/sf-airbnb-clean.parquet",
        "evaluation_experiment": "/Shared/mlops/model_evaluation",
        "evaluation_type": "comprehensive",
        "model_name": "workspace.default.california_housing_predictor",
        "model_version": "2"
       },
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run_mlflow_evaluation():\n",
    "    \"\"\"Run comprehensive model evaluation using standard MLflow methods.\"\"\"\n",
    "    \n",
    "    # Set up evaluation experiment\n",
    "    mlflow.set_experiment(EVALUATION_EXPERIMENT)\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"evaluation_{MODEL_NAME}_v{model_version_info.version}\") as run:\n",
    "        \n",
    "        # Log evaluation parameters\n",
    "        mlflow.log_param(\"model_name\", MODEL_NAME)\n",
    "        mlflow.log_param(\"model_version\", model_version_info.version)\n",
    "        mlflow.log_param(\"model_stage\", model_version_info.current_stage)\n",
    "        mlflow.log_param(\"evaluation_type\", EVALUATION_TYPE)\n",
    "        mlflow.log_param(\"test_samples\", len(X_test))\n",
    "        \n",
    "        print(\"Starting MLflow evaluation...\")\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        # Calculate standard metrics\n",
    "        mse = mean_squared_error(y_test, predictions)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test, predictions)\n",
    "        r2 = r2_score(y_test, predictions)\n",
    "        \n",
    "        # Calculate custom metrics\n",
    "        mape = np.mean(np.abs((y_test - predictions) / y_test)) * 100\n",
    "        \n",
    "        # Calculate additional metrics\n",
    "        from sklearn.metrics import explained_variance_score, max_error\n",
    "        \n",
    "        ev_score = explained_variance_score(y_test, predictions)\n",
    "        max_err = max_error(y_test, predictions)\n",
    "        \n",
    "        # Calculate prediction bounds coverage\n",
    "        min_price, max_price = 50000, 2000000\n",
    "        within_bounds = np.sum((predictions >= min_price) & (predictions <= max_price))\n",
    "        bounds_coverage = (within_bounds / len(predictions)) * 100\n",
    "        \n",
    "        # Log all metrics\n",
    "        metrics = {\n",
    "            \"test_mse\": mse,\n",
    "            \"test_rmse\": rmse,\n",
    "            \"test_mae\": mae,\n",
    "            \"test_r2\": r2,\n",
    "            \"test_mape\": mape,\n",
    "            \"test_explained_variance\": ev_score,\n",
    "            \"test_max_error\": max_err,\n",
    "            \"test_bounds_coverage\": bounds_coverage\n",
    "        }\n",
    "        \n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            mlflow.log_metric(metric_name, metric_value)\n",
    "        \n",
    "        # Prediction statistics\n",
    "        mlflow.log_metric(\"pred_mean\", float(np.mean(predictions)))\n",
    "        mlflow.log_metric(\"pred_std\", float(np.std(predictions)))\n",
    "        mlflow.log_metric(\"pred_min\", float(np.min(predictions)))\n",
    "        mlflow.log_metric(\"pred_max\", float(np.max(predictions)))\n",
    "        \n",
    "        # Residual analysis\n",
    "        residuals = y_test - predictions\n",
    "        mlflow.log_metric(\"residual_mean\", float(np.mean(residuals)))\n",
    "        mlflow.log_metric(\"residual_std\", float(np.std(residuals)))\n",
    "        \n",
    "        # Feature importance (if available)\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            feature_importance = dict(zip(X_test.columns, model.feature_importances_))\n",
    "            for feature, importance in feature_importance.items():\n",
    "                mlflow.log_metric(f\"feature_importance_{feature}\", float(importance))\n",
    "        \n",
    "        # Create and log visualizations\n",
    "        create_evaluation_plots(y_test, predictions, residuals)\n",
    "        \n",
    "        # Log sample predictions as artifact\n",
    "        sample_predictions = pd.DataFrame({\n",
    "            'actual': y_test[:100],  # First 100 samples\n",
    "            'predicted': predictions[:100],\n",
    "            'residual': residuals[:100]\n",
    "        })\n",
    "        \n",
    "        # Save to temporary file and log as artifact\n",
    "        sample_predictions.to_csv(\"/tmp/sample_predictions.csv\", index=False)\n",
    "        mlflow.log_artifact(\"/tmp/sample_predictions.csv\", \"sample_predictions.csv\")\n",
    "        \n",
    "        print(f\"Evaluation completed. Run ID: {run.info.run_id}\")\n",
    "        \n",
    "        # Create evaluation result object for compatibility\n",
    "        class EvaluationResult:\n",
    "            def __init__(self, metrics_dict):\n",
    "                self.metrics = metrics_dict\n",
    "        \n",
    "        evaluation_result = EvaluationResult(metrics)\n",
    "        \n",
    "        return evaluation_result, run.info.run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c13da908-6713-4075-b83e-5eb27480a56f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7. Evaluation Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42f44de9-d956-4616-9276-95b33420e3aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {
        "data_path": "/databricks-datasets/learning-spark-v2/sf-airbnb/sf-airbnb-clean.parquet",
        "evaluation_experiment": "/Shared/mlops/model_evaluation",
        "evaluation_type": "comprehensive",
        "model_name": "workspace.default.california_housing_predictor",
        "model_version": "2"
       },
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_evaluation_plots(y_true, y_pred, residuals):\n",
    "    \"\"\"Create evaluation plots and log them to MLflow.\"\"\"\n",
    "    \n",
    "    plt.style.use('default')\n",
    "    \n",
    "    # 1. Predictions vs Actual\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.scatter(y_true, y_pred, alpha=0.6, s=20)\n",
    "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.title('Predictions vs Actual Values')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add R score\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    plt.text(0.05, 0.95, f'R = {r2:.3f}', transform=plt.gca().transAxes, \n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # 2. Residuals plot\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.scatter(y_pred, residuals, alpha=0.6, s=20)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title('Residuals vs Predicted Values')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Residuals histogram\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.hist(residuals, bins=30, alpha=0.7, edgecolor='black')\n",
    "    plt.xlabel('Residuals')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Residuals')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Q-Q plot for residuals\n",
    "    plt.subplot(2, 2, 4)\n",
    "    try:\n",
    "        from scipy import stats\n",
    "        stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "        plt.title('Q-Q Plot of Residuals')\n",
    "    except ImportError:\n",
    "        # Fallback if scipy is not available\n",
    "        plt.hist(residuals, bins=30, alpha=0.7, edgecolor='black', density=True)\n",
    "        plt.title('Residuals Distribution (Normalized)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    mlflow.log_figure(plt.gcf(), \"evaluation_plots.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Error distribution by prediction ranges\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create prediction bins\n",
    "    n_bins = 10\n",
    "    try:\n",
    "        pred_bins = pd.cut(y_pred, bins=n_bins)\n",
    "        \n",
    "        # Calculate metrics by bin\n",
    "        bin_metrics = []\n",
    "        for bin_label in pred_bins.cat.categories:\n",
    "            mask = pred_bins == bin_label\n",
    "            if mask.sum() > 0:\n",
    "                bin_mae = mean_absolute_error(y_true[mask], y_pred[mask])\n",
    "                bin_rmse = np.sqrt(mean_squared_error(y_true[mask], y_pred[mask]))\n",
    "                bin_metrics.append({\n",
    "                    'bin': str(bin_label),\n",
    "                    'count': mask.sum(),\n",
    "                    'mae': bin_mae,\n",
    "                    'rmse': bin_rmse\n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f\"Could not create prediction bins: {e}\")\n",
    "        bin_metrics = []\n",
    "    \n",
    "    if bin_metrics:\n",
    "        metrics_df = pd.DataFrame(bin_metrics)\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.bar(range(len(metrics_df)), metrics_df['mae'])\n",
    "        plt.xlabel('Prediction Bins')\n",
    "        plt.ylabel('Mean Absolute Error')\n",
    "        plt.title('MAE by Prediction Range')\n",
    "        plt.xticks(range(len(metrics_df)), [f\"Bin {i+1}\" for i in range(len(metrics_df))], rotation=45)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.bar(range(len(metrics_df)), metrics_df['rmse'])\n",
    "        plt.xlabel('Prediction Bins')\n",
    "        plt.ylabel('Root Mean Squared Error')\n",
    "        plt.title('RMSE by Prediction Range')\n",
    "        plt.xticks(range(len(metrics_df)), [f\"Bin {i+1}\" for i in range(len(metrics_df))], rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        mlflow.log_figure(plt.gcf(), \"error_by_prediction_range.png\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc82d7bb-a8df-40ca-94ca-f5d87d192697",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 8. Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c2ef4eb-7cbf-4f51-88bc-47074286de1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {
        "data_path": "/databricks-datasets/learning-spark-v2/sf-airbnb/sf-airbnb-clean.parquet",
        "evaluation_experiment": "/Shared/mlops/model_evaluation",
        "evaluation_type": "comprehensive",
        "model_name": "workspace.default.california_housing_predictor",
        "model_version": "2"
       },
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Execute the evaluation\n",
    "evaluation_result, evaluation_run_id = run_mlflow_evaluation()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"EVALUATION COMPLETED SUCCESSFULLY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Model: {MODEL_NAME} v{model_version_info.version}\")\n",
    "print(f\"Evaluation Run ID: {evaluation_run_id}\")\n",
    "print(f\"Evaluation Experiment: {EVALUATION_EXPERIMENT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "803c44b2-ea5f-4759-a51a-b209c3d816b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 9. Evaluation Summary and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9aeb278b-d71e-4027-9692-97da3c9ea392",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {
        "data_path": "/databricks-datasets/learning-spark-v2/sf-airbnb/sf-airbnb-clean.parquet",
        "evaluation_experiment": "/Shared/mlops/model_evaluation",
        "evaluation_type": "comprehensive",
        "model_name": "workspace.default.california_housing_predictor",
        "model_version": "2"
       },
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_evaluation_summary(eval_result):\n",
    "    \"\"\"Display a comprehensive evaluation summary.\"\"\"\n",
    "    \n",
    "    print(\"EVALUATION SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Extract metrics\n",
    "    metrics = eval_result.metrics\n",
    "    \n",
    "    print(\"\\n\uD83D\uDCCA PERFORMANCE METRICS:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Core regression metrics\n",
    "    core_metrics = ['test_mse', 'test_rmse', 'test_mae', 'test_r2', 'test_max_error']\n",
    "    for metric in core_metrics:\n",
    "        if metric in metrics:\n",
    "            metric_display = metric.replace('test_', '').upper().replace('_', ' ')\n",
    "            print(f\"{metric_display}: {metrics[metric]:.4f}\")\n",
    "    \n",
    "    # Custom metrics\n",
    "    custom_metric_names = ['test_mape', 'test_explained_variance', 'test_bounds_coverage']\n",
    "    print(f\"\\n\uD83C\uDFAF CUSTOM METRICS:\")\n",
    "    print(\"-\" * 30)\n",
    "    for metric in custom_metric_names:\n",
    "        if metric in metrics:\n",
    "            if metric == 'test_mape':\n",
    "                print(f\"MEAN ABSOLUTE PERCENTAGE ERROR: {metrics[metric]:.2f}%\")\n",
    "            elif metric == 'test_bounds_coverage':\n",
    "                print(f\"PREDICTIONS WITHIN BOUNDS: {metrics[metric]:.1f}%\")\n",
    "            elif metric == 'test_explained_variance':\n",
    "                print(f\"EXPLAINED VARIANCE: {metrics[metric]:.4f}\")\n",
    "            else:\n",
    "                metric_display = metric.replace('test_', '').upper().replace('_', ' ')\n",
    "                print(f\"{metric_display}: {metrics[metric]:.4f}\")\n",
    "    \n",
    "    # Model quality assessment\n",
    "    print(f\"\\n\uD83C\uDFC6 MODEL QUALITY ASSESSMENT:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    r2 = metrics.get('test_r2', 0)\n",
    "    mape = metrics.get('test_mape', float('inf'))\n",
    "    \n",
    "    if r2 > 0.9:\n",
    "        quality = \"EXCELLENT\"\n",
    "    elif r2 > 0.8:\n",
    "        quality = \"GOOD\"\n",
    "    elif r2 > 0.7:\n",
    "        quality = \"ACCEPTABLE\"\n",
    "    else:\n",
    "        quality = \"NEEDS IMPROVEMENT\"\n",
    "    \n",
    "    print(f\"Overall Quality: {quality}\")\n",
    "    print(f\"R Score: {r2:.3f}\")\n",
    "    \n",
    "    if mape != float('inf'):\n",
    "        if mape < 10:\n",
    "            mape_assessment = \"EXCELLENT\"\n",
    "        elif mape < 20:\n",
    "            mape_assessment = \"GOOD\"\n",
    "        elif mape < 30:\n",
    "            mape_assessment = \"ACCEPTABLE\"\n",
    "        else:\n",
    "            mape_assessment = \"POOR\"\n",
    "        print(f\"MAPE Assessment: {mape_assessment} ({mape:.1f}%)\")\n",
    "    \n",
    "    return quality, metrics\n",
    "\n",
    "# Display the summary\n",
    "model_quality, eval_metrics = display_evaluation_summary(evaluation_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b52c6b82-a924-4a27-b54b-ed165ae5ba5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 10. Evaluation Decision and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ef51e78-2af4-437b-a52a-b55d721aa973",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {
        "data_path": "/databricks-datasets/learning-spark-v2/sf-airbnb/sf-airbnb-clean.parquet",
        "evaluation_experiment": "/Shared/mlops/model_evaluation",
        "evaluation_type": "comprehensive",
        "model_name": "workspace.default.california_housing_predictor",
        "model_version": "2"
       },
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def make_evaluation_decision(quality: str, metrics: Dict[str, float]) -> Dict[str, Any]:\n",
    "    \"\"\"Make evaluation decision based on metrics.\"\"\"\n",
    "    \n",
    "    # Define thresholds\n",
    "    thresholds = {\n",
    "        'min_r2_score': 0.7,\n",
    "        'max_mape': 25.0,\n",
    "        'min_bounds_coverage': 95.0\n",
    "    }\n",
    "    \n",
    "    # Check each criterion\n",
    "    r2_score = metrics.get('test_r2', 0)\n",
    "    mape = metrics.get('test_mape', float('inf'))\n",
    "    bounds_coverage = metrics.get('test_bounds_coverage', 0)\n",
    "    \n",
    "    passed_checks = []\n",
    "    failed_checks = []\n",
    "    \n",
    "    # R Score check\n",
    "    if r2_score >= thresholds['min_r2_score']:\n",
    "        passed_checks.append(f\"R Score: {r2_score:.3f} >= {thresholds['min_r2_score']}\")\n",
    "    else:\n",
    "        failed_checks.append(f\"R Score: {r2_score:.3f} < {thresholds['min_r2_score']}\")\n",
    "    \n",
    "    # MAPE check\n",
    "    if mape <= thresholds['max_mape']:\n",
    "        passed_checks.append(f\"MAPE: {mape:.1f}% <= {thresholds['max_mape']}%\")\n",
    "    else:\n",
    "        failed_checks.append(f\"MAPE: {mape:.1f}% > {thresholds['max_mape']}%\")\n",
    "    \n",
    "    # Bounds coverage check\n",
    "    if bounds_coverage >= thresholds['min_bounds_coverage']:\n",
    "        passed_checks.append(f\"Bounds Coverage: {bounds_coverage:.1f}% >= {thresholds['min_bounds_coverage']}%\")\n",
    "    else:\n",
    "        failed_checks.append(f\"Bounds Coverage: {bounds_coverage:.1f}% < {thresholds['min_bounds_coverage']}%\")\n",
    "    \n",
    "    # Make decision\n",
    "    passes_evaluation = len(failed_checks) == 0\n",
    "    \n",
    "    decision = {\n",
    "        'passes_evaluation': passes_evaluation,\n",
    "        'recommendation': 'APPROVE' if passes_evaluation else 'REJECT',\n",
    "        'quality_score': quality,\n",
    "        'passed_checks': passed_checks,\n",
    "        'failed_checks': failed_checks,\n",
    "        'thresholds_used': thresholds,\n",
    "        'evaluation_timestamp': pd.Timestamp.now().isoformat(),\n",
    "        'model_name': MODEL_NAME,\n",
    "        'model_version': model_version_info.version\n",
    "    }\n",
    "    \n",
    "    return decision\n",
    "\n",
    "# Make evaluation decision\n",
    "evaluation_decision = make_evaluation_decision(model_quality, eval_metrics)\n",
    "\n",
    "# Log decision to MLflow\n",
    "with mlflow.start_run(run_id=evaluation_run_id):\n",
    "    mlflow.log_param(\"evaluation_decision\", evaluation_decision['recommendation'])\n",
    "    mlflow.log_param(\"evaluation_quality\", evaluation_decision['quality_score'])\n",
    "    mlflow.log_metric(\"passes_evaluation\", 1.0 if evaluation_decision['passes_evaluation'] else 0.0)\n",
    "    \n",
    "    # Log decision details as JSON\n",
    "    import json\n",
    "    mlflow.log_text(json.dumps(evaluation_decision, indent=2), \"evaluation_decision.json\")\n",
    "\n",
    "# Display decision\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"EVALUATION DECISION\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Model: {MODEL_NAME} v{model_version_info.version}\")\n",
    "print(f\"Recommendation: {evaluation_decision['recommendation']}\")\n",
    "print(f\"Quality Score: {evaluation_decision['quality_score']}\")\n",
    "\n",
    "if evaluation_decision['passed_checks']:\n",
    "    print(f\"\\n PASSED CHECKS:\")\n",
    "    for check in evaluation_decision['passed_checks']:\n",
    "        print(f\"   {check}\")\n",
    "\n",
    "if evaluation_decision['failed_checks']:\n",
    "    print(f\"\\n FAILED CHECKS:\")\n",
    "    for check in evaluation_decision['failed_checks']:\n",
    "        print(f\"   {check}\")\n",
    "\n",
    "print(f\"\\n\uD83D\uDCCB NEXT STEPS:\")\n",
    "if evaluation_decision['passes_evaluation']:\n",
    "    print(\"  1. Model is ready for approval review\")\n",
    "    print(\"  2. Run the approval notebook for human review\")\n",
    "    print(\"  3. If approved, proceed to deployment\")\n",
    "else:\n",
    "    print(\"  1. Model requires improvement before approval\")\n",
    "    print(\"  2. Review failed checks and retrain model\")\n",
    "    print(\"  3. Re-run evaluation after improvements\")\n",
    "\n",
    "print(f\"\\n\uD83D\uDCCA Evaluation Run ID: {evaluation_run_id}\")\n",
    "print(f\"\uD83D\uDD17 View results in MLflow UI: {EVALUATION_EXPERIMENT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "703c59ed-ca70-4fce-8537-b736c0a5fb23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 11. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53f3a53e-092f-4aec-ae94-74aa94f61714",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {
        "data_path": "/databricks-datasets/learning-spark-v2/sf-airbnb/sf-airbnb-clean.parquet",
        "evaluation_experiment": "/Shared/mlops/model_evaluation",
        "evaluation_type": "comprehensive",
        "model_name": "workspace.default.california_housing_predictor",
        "model_version": "2"
       },
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create evaluation summary for next stage\n",
    "evaluation_summary = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'model_version': model_version_info.version,\n",
    "    'evaluation_run_id': evaluation_run_id,\n",
    "    'evaluation_decision': evaluation_decision,\n",
    "    'metrics': eval_metrics,\n",
    "    'experiment_name': EVALUATION_EXPERIMENT,\n",
    "    'evaluation_type': EVALUATION_TYPE,\n",
    "    'test_data_samples': len(X_test)\n",
    "}\n",
    "\n",
    "# Save to DBFS for next notebook\n",
    "import json\n",
    "dbutils.fs.put(\"/tmp/model_evaluation_results.json\", \n",
    "               json.dumps(evaluation_summary, indent=2, default=str))\n",
    "\n",
    "print(\" Evaluation results exported to /tmp/model_evaluation_results.json\")\n",
    "print(\"\uD83D\uDE80 Ready for approval workflow!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_model_evaluation (1)",
   "widgets": {
    "data_path": {
     "currentValue": "/databricks-datasets/learning-spark-v2/sf-airbnb/sf-airbnb-clean.parquet",
     "nuid": "0a6c6de9-33eb-4b56-80c8-2a6804c90bc3",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/databricks-datasets/learning-spark-v2/sf-airbnb/sf-airbnb-clean.parquet",
      "label": "Test Data Path",
      "name": "data_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/databricks-datasets/learning-spark-v2/sf-airbnb/sf-airbnb-clean.parquet",
      "label": "Test Data Path",
      "name": "data_path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "evaluation_experiment": {
     "currentValue": "/Shared/mlops/model_evaluation",
     "nuid": "731f60d1-9222-4f72-9447-6926ddc683ad",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/Shared/mlops/model_evaluation",
      "label": "Evaluation Experiment Path",
      "name": "evaluation_experiment",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/Shared/mlops/model_evaluation",
      "label": "Evaluation Experiment Path",
      "name": "evaluation_experiment",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "evaluation_type": {
     "currentValue": "comprehensive",
     "nuid": "9460081e-e214-4781-90de-b0f3c950cdea",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "comprehensive",
      "label": "Evaluation Type",
      "name": "evaluation_type",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "quick",
        "comprehensive"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "comprehensive",
      "label": "Evaluation Type",
      "name": "evaluation_type",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "quick",
        "comprehensive"
       ]
      }
     }
    },
    "model_name": {
     "currentValue": "workspace.default.california_housing_predictor",
     "nuid": "f0ed75b3-5a47-4a22-b88f-d653f811d3ec",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "california_housing_predictor",
      "label": "Registered Model Name",
      "name": "model_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "california_housing_predictor",
      "label": "Registered Model Name",
      "name": "model_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "model_version": {
     "currentValue": "2",
     "nuid": "403ed953-a0b7-49a3-8cf7-c10762006cf3",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "latest",
      "label": "Model Version (or 'latest')",
      "name": "model_version",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "latest",
      "label": "Model Version (or 'latest')",
      "name": "model_version",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}