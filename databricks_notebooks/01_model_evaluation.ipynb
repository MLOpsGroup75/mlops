{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb1d6fb9-61e2-4be7-b0d2-7a8cd522b1c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Model Evaluation Task\n",
    "\n",
    "This notebook defines the evaluation task for the California Housing MLOps pipeline.\n",
    "It uses standard MLflow methods to produce comprehensive validation metrics for model versions.\n",
    "\n",
    "## Features:\n",
    "- Automated model evaluation using standard MLflow metrics\n",
    "- Custom metrics and visualizations\n",
    "- Evaluation results logging\n",
    "- Model performance comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27d9ed18-dc33-4a80-939c-358e1692f32a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9b36006-d61e-45a6-aa39-b12fef37bd15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade typing_extensions mlflow\n",
    "\n",
    "# Import required libraries\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "# Note: mlflow.evaluate requires MLflow 2.0+\n",
    "# Using standard MLflow evaluation for compatibility\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mlflow.tracking import MlflowClient\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from typing import Dict, Any, List, Optional\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e1a4164-cb6d-45e3-8f73-80d45907001c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b4cd04f-2dca-44d4-8ab3-83fc633e3698",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "dbutils.widgets.text(\"model_name\", \"california_housing_predictor\", \"Registered Model Name\")\n",
    "dbutils.widgets.text(\"model_version\", \"latest\", \"Model Version (or 'latest')\")\n",
    "dbutils.widgets.text(\"evaluation_experiment\", \"/Shared/mlops/model_evaluation\", \"Evaluation Experiment Path\")\n",
    "dbutils.widgets.text(\"data_path\", \"/databricks-datasets/learning-spark-v2/sf-airbnb/sf-airbnb-clean.parquet\", \"Test Data Path\")\n",
    "dbutils.widgets.dropdown(\"evaluation_type\", \"comprehensive\", [\"quick\", \"comprehensive\"], \"Evaluation Type\")\n",
    "\n",
    "# Get parameters\n",
    "MODEL_NAME = dbutils.widgets.get(\"model_name\")\n",
    "MODEL_VERSION = dbutils.widgets.get(\"model_version\") \n",
    "EVALUATION_EXPERIMENT = dbutils.widgets.get(\"evaluation_experiment\")\n",
    "DATA_PATH = dbutils.widgets.get(\"data_path\")\n",
    "EVALUATION_TYPE = dbutils.widgets.get(\"evaluation_type\")\n",
    "\n",
    "print(f\"Model Name: {MODEL_NAME}\")\n",
    "print(f\"Model Version: {MODEL_VERSION}\")\n",
    "print(f\"Evaluation Experiment: {EVALUATION_EXPERIMENT}\")\n",
    "print(f\"Evaluation Type: {EVALUATION_TYPE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "924e39ad-b991-4efc-a6ff-1b5cc2c467d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "434fcb7a-d28c-4adc-99e5-276c82caa4cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_test_data():\n",
    "    \"\"\"Load test data for evaluation.\"\"\"\n",
    "    try:\n",
    "        # For California Housing dataset, try to load from DBFS or local source\n",
    "        # This is a placeholder - adapt based on your actual data location\n",
    "        \n",
    "        # Option 1: Load from DBFS if uploaded\n",
    "        try:\n",
    "            df = spark.read.csv(\"/databricks-datasets/california-housing/cal_housing.data\", \n",
    "                              header=False, inferSchema=True)\n",
    "            # California Housing column names\n",
    "            columns = ['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n",
    "                      'total_bedrooms', 'population', 'households', 'median_income', 'median_house_value']\n",
    "            \n",
    "            for i, col_name in enumerate(columns):\n",
    "                df = df.withColumnRenamed(f\"_c{i}\", col_name)\n",
    "            \n",
    "            # Convert to Pandas for sklearn compatibility\n",
    "            df_pandas = df.toPandas()\n",
    "            \n",
    "            # Prepare features and target\n",
    "            X = df_pandas.drop('median_house_value', axis=1)\n",
    "            y = df_pandas['median_house_value']\n",
    "            \n",
    "            print(f\"Loaded test data: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "            return X, y\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Could not load from DBFS datasets: {e}\")\n",
    "            \n",
    "            # Option 2: Generate synthetic test data for demonstration\n",
    "            print(\"Generating synthetic test data...\")\n",
    "            np.random.seed(42)\n",
    "            n_samples = 1000\n",
    "            \n",
    "            X = pd.DataFrame({\n",
    "                'longitude': np.random.uniform(-124, -114, n_samples),\n",
    "                'latitude': np.random.uniform(32, 42, n_samples),\n",
    "                'housing_median_age': np.random.uniform(1, 52, n_samples),\n",
    "                'total_rooms': np.random.uniform(1, 10000, n_samples),\n",
    "                'total_bedrooms': np.random.uniform(1, 2000, n_samples),\n",
    "                'population': np.random.uniform(1, 8000, n_samples),\n",
    "                'households': np.random.uniform(1, 2000, n_samples),\n",
    "                'median_income': np.random.uniform(0.5, 15, n_samples)\n",
    "            })\n",
    "            \n",
    "            # Generate target with some relationship to features\n",
    "            y = (X['median_income'] * 50000 + \n",
    "                 X['total_rooms'] * 10 + \n",
    "                 np.random.normal(0, 50000, n_samples))\n",
    "            y = np.abs(y)  # Ensure positive prices\n",
    "            \n",
    "            print(f\"Generated synthetic test data: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "            return X, y\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading test data: {e}\")\n",
    "        raise\n",
    "\n",
    "# Load test data\n",
    "X_test, y_test = load_test_data()\n",
    "\n",
    "# Display data info\n",
    "print(\"\\nTest Data Overview:\")\n",
    "print(f\"Features shape: {X_test.shape}\")\n",
    "print(f\"Target shape: {y_test.shape}\")\n",
    "print(f\"Features: {list(X_test.columns)}\")\n",
    "print(f\"Target range: {y_test.min():.2f} - {y_test.max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef996a5b-e44b-4080-8f5b-669ae8a43038",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Model Loading and Basic Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eab52b24-80c2-4cbb-866f-b94a9fcdf2d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"MLflow version: {mlflow.__version__}\")\n",
    "import sklearn\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
    "\n",
    "\n",
    "from typing import Any\n",
    "def load_model_for_evaluation(model_name: str, version: str) -> Any:\n",
    "\n",
    "    \"\"\"Load model from MLflow Model Registry.\"\"\"\n",
    "    try:\n",
    "        from mlflow.tracking import MlflowClient\n",
    "        import mlflow.sklearn\n",
    "        import logging\n",
    "\n",
    "        logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Get model version\n",
    "        if version.lower() == \"latest\":\n",
    "            # Get latest version\n",
    "            client = MlflowClient()\n",
    "            latest_versions = client.get_latest_versions(model_name, stages=[\"None\", \"Staging\", \"Production\"])\n",
    "            if not latest_versions:\n",
    "                raise ValueError(f\"No versions found for model {model_name}\")\n",
    "            \n",
    "            # Sort by version number and get the latest\n",
    "            latest_version = max(latest_versions, key=lambda x: int(x.version))\n",
    "            version = latest_version.version\n",
    "            print(f\"Using latest version: {version}\")\n",
    "        \n",
    "        # Load model\n",
    "        model_uri = f\"models:/{model_name}/{version}\"\n",
    "        model = mlflow.sklearn.load_model(model_uri)\n",
    "        \n",
    "        # Get model version info\n",
    "        model_version = client.get_model_version(model_name, version)\n",
    "        \n",
    "        print(f\"Loaded model: {model_name} version {version}\")\n",
    "        print(f\"Model stage: {model_version.current_stage}\")\n",
    "        print(f\"Model description: {model_version.description}\")\n",
    "        \n",
    "        return model, model_version\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading model: {e}\")\n",
    "        raise\n",
    "\n",
    "# Load model\n",
    "model, model_version_info = load_model_for_evaluation(MODEL_NAME, MODEL_VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76e898c7-2f06-47e1-96ce-539568a816b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Custom Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "699253b5-0f99-4017-b3b0-6ba726cd5916",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {
        "data_path": "/databricks-datasets/learning-spark-v2/sf-airbnb/sf-airbnb-clean.parquet",
        "evaluation_experiment": "/Shared/mlops/model_evaluation",
        "evaluation_type": "comprehensive",
        "model_name": "workspace.default.california_housing_predictor",
        "model_version": "2"
       },
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def custom_metrics():\n",
    "    \"\"\"Define custom metrics for evaluation (kept for reference).\"\"\"\n",
    "    \n",
    "    def mean_absolute_percentage_error(y_true, y_pred):\n",
    "        \"\"\"Calculate Mean Absolute Percentage Error.\"\"\"\n",
    "        return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    \n",
    "    def root_mean_squared_error(y_true, y_pred):\n",
    "        \"\"\"Calculate Root Mean Squared Error.\"\"\"\n",
    "        return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    \n",
    "    def explained_variance_score(y_true, y_pred):\n",
    "        \"\"\"Calculate Explained Variance Score.\"\"\"\n",
    "        from sklearn.metrics import explained_variance_score as evs\n",
    "        return evs(y_true, y_pred)\n",
    "    \n",
    "    def prediction_bounds_coverage(y_true, y_pred):\n",
    "        \"\"\"Calculate percentage of predictions within reasonable bounds.\"\"\"\n",
    "        # Define reasonable bounds for California housing prices\n",
    "        min_price, max_price = 50000, 2000000\n",
    "        within_bounds = np.sum((y_pred >= min_price) & (y_pred <= max_price))\n",
    "        return (within_bounds / len(y_pred)) * 100\n",
    "    \n",
    "    return {\n",
    "        \"mape\": mean_absolute_percentage_error,\n",
    "        \"rmse\": root_mean_squared_error,\n",
    "        \"explained_variance\": explained_variance_score,\n",
    "        \"bounds_coverage\": prediction_bounds_coverage\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29202a7d-56a5-4d1e-9b74-182c529ed26c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. MLflow Evaluation Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c55fa244-175f-4791-8d96-b8897a9f08f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {
        "data_path": "/databricks-datasets/learning-spark-v2/sf-airbnb/sf-airbnb-clean.parquet",
        "evaluation_experiment": "/Shared/mlops/model_evaluation",
        "evaluation_type": "comprehensive",
        "model_name": "workspace.default.california_housing_predictor",
        "model_version": "2"
       },
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run_mlflow_evaluation():\n",
    "    \"\"\"Run comprehensive model evaluation using standard MLflow methods.\"\"\"\n",
    "    \n",
    "    # Set up evaluation experiment\n",
    "    mlflow.set_experiment(EVALUATION_EXPERIMENT)\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"evaluation_{MODEL_NAME}_v{model_version_info.version}\") as run:\n",
    "        \n",
    "        # Log evaluation parameters\n",
    "        mlflow.log_param(\"model_name\", MODEL_NAME)\n",
    "        mlflow.log_param(\"model_version\", model_version_info.version)\n",
    "        mlflow.log_param(\"model_stage\", model_version_info.current_stage)\n",
    "        mlflow.log_param(\"evaluation_type\", EVALUATION_TYPE)\n",
    "        mlflow.log_param(\"test_samples\", len(X_test))\n",
    "        \n",
    "        print(\"Starting MLflow evaluation...\")\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        # Calculate standard metrics\n",
    "        mse = mean_squared_error(y_test, predictions)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test, predictions)\n",
    "        r2 = r2_score(y_test, predictions)\n",
    "        \n",
    "        # Calculate custom metrics\n",
    "        mape = np.mean(np.abs((y_test - predictions) / y_test)) * 100\n",
    "        \n",
    "        # Calculate additional metrics\n",
    "        from sklearn.metrics import explained_variance_score, max_error\n",
    "        \n",
    "        ev_score = explained_variance_score(y_test, predictions)\n",
    "        max_err = max_error(y_test, predictions)\n",
    "        \n",
    "        # Calculate prediction bounds coverage\n",
    "        min_price, max_price = 50000, 2000000\n",
    "        within_bounds = np.sum((predictions >= min_price) & (predictions <= max_price))\n",
    "        bounds_coverage = (within_bounds / len(predictions)) * 100\n",
    "        \n",
    "        # Log all metrics\n",
    "        metrics = {\n",
    "            \"test_mse\": mse,\n",
    "            \"test_rmse\": rmse,\n",
    "            \"test_mae\": mae,\n",
    "            \"test_r2\": r2,\n",
    "            \"test_mape\": mape,\n",
    "            \"test_explained_variance\": ev_score,\n",
    "            \"test_max_error\": max_err,\n",
    "            \"test_bounds_coverage\": bounds_coverage\n",
    "        }\n",
    "        \n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            mlflow.log_metric(metric_name, metric_value)\n",
    "        \n",
    "        # Prediction statistics\n",
    "        mlflow.log_metric(\"pred_mean\", float(np.mean(predictions)))\n",
    "        mlflow.log_metric(\"pred_std\", float(np.std(predictions)))\n",
    "        mlflow.log_metric(\"pred_min\", float(np.min(predictions)))\n",
    "        mlflow.log_metric(\"pred_max\", float(np.max(predictions)))\n",
    "        \n",
    "        # Residual analysis\n",
    "        residuals = y_test - predictions\n",
    "        mlflow.log_metric(\"residual_mean\", float(np.mean(residuals)))\n",
    "        mlflow.log_metric(\"residual_std\", float(np.std(residuals)))\n",
    "        \n",
    "        # Feature importance (if available)\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            feature_importance = dict(zip(X_test.columns, model.feature_importances_))\n",
    "            for feature, importance in feature_importance.items():\n",
    "                mlflow.log_metric(f\"feature_importance_{feature}\", float(importance))\n",
    "        \n",
    "        # Create and log visualizations\n",
    "        create_evaluation_plots(y_test, predictions, residuals)\n",
    "        \n",
    "        # Log sample predictions as artifact\n",
    "        sample_predictions = pd.DataFrame({\n",
    "            'actual': y_test[:100],  # First 100 samples\n",
    "            'predicted': predictions[:100],\n",
    "            'residual': residuals[:100]\n",
    "        })\n",
    "        \n",
    "        # Save to temporary file and log as artifact\n",
    "        sample_predictions.to_csv(\"/tmp/sample_predictions.csv\", index=False)\n",
    "        mlflow.log_artifact(\"/tmp/sample_predictions.csv\", \"sample_predictions.csv\")\n",
    "        \n",
    "        print(f\"Evaluation completed. Run ID: {run.info.run_id}\")\n",
    "        \n",
    "        # Create evaluation result object for compatibility\n",
    "        class EvaluationResult:\n",
    "            def __init__(self, metrics_dict):\n",
    "                self.metrics = metrics_dict\n",
    "        \n",
    "        evaluation_result = EvaluationResult(metrics)\n",
    "        \n",
    "        return evaluation_result, run.info.run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c13da908-6713-4075-b83e-5eb27480a56f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7. Evaluation Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42f44de9-d956-4616-9276-95b33420e3aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {
        "data_path": "/databricks-datasets/learning-spark-v2/sf-airbnb/sf-airbnb-clean.parquet",
        "evaluation_experiment": "/Shared/mlops/model_evaluation",
        "evaluation_type": "comprehensive",
        "model_name": "workspace.default.california_housing_predictor",
        "model_version": "2"
       },
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_evaluation_plots(y_true, y_pred, residuals):\n",
    "    \"\"\"Create evaluation plots and log them to MLflow.\"\"\"\n",
    "    \n",
    "    plt.style.use('default')\n",
    "    \n",
    "    # 1. Predictions vs Actual\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.scatter(y_true, y_pred, alpha=0.6, s=20)\n",
    "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.title('Predictions vs Actual Values')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add RÂ² score\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    plt.text(0.05, 0.95, f'RÂ² = {r2:.3f}', transform=plt.gca().transAxes, \n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # 2. Residuals plot\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.scatter(y_pred, residuals, alpha=0.6, s=20)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title('Residuals vs Predicted Values')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Residuals histogram\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.hist(residuals, bins=30, alpha=0.7, edgecolor='black')\n",
    "    plt.xlabel('Residuals')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Residuals')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Q-Q plot for residuals\n",
    "    plt.subplot(2, 2, 4)\n",
    "    try:\n",
    "        from scipy import stats\n",
    "        stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "        plt.title('Q-Q Plot of Residuals')\n",
    "    except ImportError:\n",
    "        # Fallback if scipy is not available\n",
    "        plt.hist(residuals, bins=30, alpha=0.7, edgecolor='black', density=True)\n",
    "        plt.title('Residuals Distribution (Normalized)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    mlflow.log_figure(plt.gcf(), \"evaluation_plots.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Error distribution by prediction ranges\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create prediction bins\n",
    "    n_bins = 10\n",
    "    try:\n",
    "        pred_bins = pd.cut(y_pred, bins=n_bins)\n",
    "        \n",
    "        # Calculate metrics by bin\n",
    "        bin_metrics = []\n",
    "        for bin_label in pred_bins.cat.categories:\n",
    "            mask = pred_bins == bin_label\n",
    "            if mask.sum() > 0:\n",
    "                bin_mae = mean_absolute_error(y_true[mask], y_pred[mask])\n",
    "                bin_rmse = np.sqrt(mean_squared_error(y_true[mask], y_pred[mask]))\n",
    "                bin_metrics.append({\n",
    "                    'bin': str(bin_label),\n",
    "                    'count': mask.sum(),\n",
    "                    'mae': bin_mae,\n",
    "                    'rmse': bin_rmse\n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f\"Could not create prediction bins: {e}\")\n",
    "        bin_metrics = []\n",
    "    \n",
    "    if bin_metrics:\n",
    "        metrics_df = pd.DataFrame(bin_metrics)\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.bar(range(len(metrics_df)), metrics_df['mae'])\n",
    "        plt.xlabel('Prediction Bins')\n",
    "        plt.ylabel('Mean Absolute Error')\n",
    "        plt.title('MAE by Prediction Range')\n",
    "        plt.xticks(range(len(metrics_df)), [f\"Bin {i+1}\" for i in range(len(metrics_df))], rotation=45)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.bar(range(len(metrics_df)), metrics_df['rmse'])\n",
    "        plt.xlabel('Prediction Bins')\n",
    "        plt.ylabel('Root Mean Squared Error')\n",
    "        plt.title('RMSE by Prediction Range')\n",
    "        plt.xticks(range(len(metrics_df)), [f\"Bin {i+1}\" for i in range(len(metrics_df))], rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        mlflow.log_figure(plt.gcf(), \"error_by_prediction_range.png\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc82d7bb-a8df-40ca-94ca-f5d87d192697",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 8. Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c2ef4eb-7cbf-4f51-88bc-47074286de1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {
        "data_path": "/databricks-datasets/learning-spark-v2/sf-airbnb/sf-airbnb-clean.parquet",
        "evaluation_experiment": "/Shared/mlops/model_evaluation",
        "evaluation_type": "comprehensive",
        "model_name": "workspace.default.california_housing_predictor",
        "model_version": "2"
       },
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Execute the evaluation\n",
    "evaluation_result, evaluation_run_id = run_mlflow_evaluation()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"EVALUATION COMPLETED SUCCESSFULLY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Model: {MODEL_NAME} v{model_version_info.version}\")\n",
    "print(f\"Evaluation Run ID: {evaluation_run_id}\")\n",
    "print(f\"Evaluation Experiment: {EVALUATION_EXPERIMENT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "803c44b2-ea5f-4759-a51a-b209c3d816b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 9. Evaluation Summary and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9aeb278b-d71e-4027-9692-97da3c9ea392",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {
        "data_path": "/databricks-datasets/learning-spark-v2/sf-airbnb/sf-airbnb-clean.parquet",
        "evaluation_experiment": "/Shared/mlops/model_evaluation",
        "evaluation_type": "comprehensive",
        "model_name": "workspace.default.california_housing_predictor",
        "model_version": "2"
       },
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_evaluation_summary(eval_result):\n",
    "    \"\"\"Display a comprehensive evaluation summary.\"\"\"\n",
    "    \n",
    "    print(\"EVALUATION SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Extract metrics\n",
    "    metrics = eval_result.metrics\n",
    "    \n",
    "    print(\"\\nðŸ“Š PERFORMANCE METRICS:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Core regression metrics\n",
    "    core_metrics = ['test_mse', 'test_rmse', 'test_mae', 'test_r2', 'test_max_error']\n",
    "    for metric in core_metrics:\n",
    "        if metric in metrics:\n",
    "            metric_display = metric.replace('test_', '').upper().replace('_', ' ')\n",
    "            print(f\"{metric_display}: {metrics[metric]:.4f}\")\n",
    "    \n",
    "    # Custom metrics\n",
    "    custom_metric_names = ['test_mape', 'test_explained_variance', 'test_bounds_coverage']\n",
    "    print(f\"\\nðŸŽ¯ CUSTOM METRICS:\")\n",
    "    print(\"-\" * 30)\n",
    "    for metric in custom_metric_names:\n",
    "        if metric in metrics:\n",
    "            if metric == 'test_mape':\n",
    "                print(f\"MEAN ABSOLUTE PERCENTAGE ERROR: {metrics[metric]:.2f}%\")\n",
    "            elif metric == 'test_bounds_coverage':\n",
    "                print(f\"PREDICTIONS WITHIN BOUNDS: {metrics[metric]:.1f}%\")\n",
    "            elif metric == 'test_explained_variance':\n",
    "                print(f\"EXPLAINED VARIANCE: {metrics[metric]:.4f}\")\n",
    "            else:\n",
    "                metric_display = metric.replace('test_', '').upper().replace('_', ' ')\n",
    "                print(f\"{metric_display}: {metrics[metric]:.4f}\")\n",
    "    \n",
    "    # Model quality assessment\n",
    "    print(f\"\\nðŸ† MODEL QUALITY ASSESSMENT:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    r2 = metrics.get('test_r2', 0)\n",
    "    mape = metrics.get('test_mape', float('inf'))\n",
    "    \n",
    "    if r2 > 0.9:\n",
    "        quality = \"EXCELLENT\"\n",
    "    elif r2 > 0.8:\n",
    "        quality = \"GOOD\"\n",
    "    elif r2 > 0.7:\n",
    "        quality = \"ACCEPTABLE\"\n",
    "    else:\n",
    "        quality = \"NEEDS IMPROVEMENT\"\n",
    "    \n",
    "    print(f\"Overall Quality: {quality}\")\n",
    "    print(f\"RÂ² Score: {r2:.3f}\")\n",
    "    \n",
    "    if mape != float('inf'):\n",
    "        if mape < 10:\n",
    "            mape_assessment = \"EXCELLENT\"\n",
    "        elif mape < 20:\n",
    "            mape_assessment = \"GOOD\"\n",
    "        elif mape < 30:\n",
    "            mape_assessment = \"ACCEPTABLE\"\n",
    "        else:\n",
    "            mape_assessment = \"POOR\"\n",
    "        print(f\"MAPE Assessment: {mape_assessment} ({mape:.1f}%)\")\n",
    "    \n",
    "    return quality, metrics\n",
    "\n",
    "# Display the summary\n",
    "model_quality, eval_metrics = display_evaluation_summary(evaluation_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b52c6b82-a924-4a27-b54b-ed165ae5ba5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 10. Evaluation Decision and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ef51e78-2af4-437b-a52a-b55d721aa973",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {
        "data_path": "/databricks-datasets/learning-spark-v2/sf-airbnb/sf-airbnb-clean.parquet",
        "evaluation_experiment": "/Shared/mlops/model_evaluation",
        "evaluation_type": "comprehensive",
        "model_name": "workspace.default.california_housing_predictor",
        "model_version": "2"
       },
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def make_evaluation_decision(quality: str, metrics: Dict[str, float]) -> Dict[str, Any]:\n",
    "    \"\"\"Make evaluation decision based on metrics.\"\"\"\n",
    "    \n",
    "    # Define thresholds\n",
    "    thresholds = {\n",
    "        'min_r2_score': 0.7,\n",
    "        'max_mape': 25.0,\n",
    "        'min_bounds_coverage': 95.0\n",
    "    }\n",
    "    \n",
    "    # Check each criterion\n",
    "    r2_score = metrics.get('test_r2', 0)\n",
    "    mape = metrics.get('test_mape', float('inf'))\n",
    "    bounds_coverage = metrics.get('test_bounds_coverage', 0)\n",
    "    \n",
    "    passed_checks = []\n",
    "    failed_checks = []\n",
    "    \n",
    "    # RÂ² Score check\n",
    "    if r2_score >= thresholds['min_r2_score']:\n",
    "        passed_checks.append(f\"RÂ² Score: {r2_score:.3f} >= {thresholds['min_r2_score']}\")\n",
    "    else:\n",
    "        failed_checks.append(f\"RÂ² Score: {r2_score:.3f} < {thresholds['min_r2_score']}\")\n",
    "    \n",
    "    # MAPE check\n",
    "    if mape <= thresholds['max_mape']:\n",
    "        passed_checks.append(f\"MAPE: {mape:.1f}% <= {thresholds['max_mape']}%\")\n",
    "    else:\n",
    "        failed_checks.append(f\"MAPE: {mape:.1f}% > {thresholds['max_mape']}%\")\n",
    "    \n",
    "    # Bounds coverage check\n",
    "    if bounds_coverage >= thresholds['min_bounds_coverage']:\n",
    "        passed_checks.append(f\"Bounds Coverage: {bounds_coverage:.1f}% >= {thresholds['min_bounds_coverage']}%\")\n",
    "    else:\n",
    "        failed_checks.append(f\"Bounds Coverage: {bounds_coverage:.1f}% < {thresholds['min_bounds_coverage']}%\")\n",
    "    \n",
    "    # Make decision\n",
    "    passes_evaluation = len(failed_checks) == 0\n",
    "    \n",
    "    decision = {\n",
    "        'passes_evaluation': passes_evaluation,\n",
    "        'recommendation': 'APPROVE' if passes_evaluation else 'REJECT',\n",
    "        'quality_score': quality,\n",
    "        'passed_checks': passed_checks,\n",
    "        'failed_checks': failed_checks,\n",
    "        'thresholds_used': thresholds,\n",
    "        'evaluation_timestamp': pd.Timestamp.now().isoformat(),\n",
    "        'model_name': MODEL_NAME,\n",
    "        'model_version': model_version_info.version\n",
    "    }\n",
    "    \n",
    "    return decision\n",
    "\n",
    "# Make evaluation decision\n",
    "evaluation_decision = make_evaluation_decision(model_quality, eval_metrics)\n",
    "\n",
    "# Log decision to MLflow\n",
    "with mlflow.start_run(run_id=evaluation_run_id):\n",
    "    mlflow.log_param(\"evaluation_decision\", evaluation_decision['recommendation'])\n",
    "    mlflow.log_param(\"evaluation_quality\", evaluation_decision['quality_score'])\n",
    "    mlflow.log_metric(\"passes_evaluation\", 1.0 if evaluation_decision['passes_evaluation'] else 0.0)\n",
    "    \n",
    "    # Log decision details as JSON\n",
    "    import json\n",
    "    mlflow.log_text(json.dumps(evaluation_decision, indent=2), \"evaluation_decision.json\")\n",
    "\n",
    "# Display decision\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"EVALUATION DECISION\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Model: {MODEL_NAME} v{model_version_info.version}\")\n",
    "print(f\"Recommendation: {evaluation_decision['recommendation']}\")\n",
    "print(f\"Quality Score: {evaluation_decision['quality_score']}\")\n",
    "\n",
    "if evaluation_decision['passed_checks']:\n",
    "    print(f\"\\nâœ… PASSED CHECKS:\")\n",
    "    for check in evaluation_decision['passed_checks']:\n",
    "        print(f\"  â€¢ {check}\")\n",
    "\n",
    "if evaluation_decision['failed_checks']:\n",
    "    print(f\"\\nâŒ FAILED CHECKS:\")\n",
    "    for check in evaluation_decision['failed_checks']:\n",
    "        print(f\"  â€¢ {check}\")\n",
    "\n",
    "print(f\"\\nðŸ“‹ NEXT STEPS:\")\n",
    "if evaluation_decision['passes_evaluation']:\n",
    "    print(\"  1. Model is ready for approval review\")\n",
    "    print(\"  2. Run the approval notebook for human review\")\n",
    "    print(\"  3. If approved, proceed to deployment\")\n",
    "else:\n",
    "    print(\"  1. Model requires improvement before approval\")\n",
    "    print(\"  2. Review failed checks and retrain model\")\n",
    "    print(\"  3. Re-run evaluation after improvements\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Evaluation Run ID: {evaluation_run_id}\")\n",
    "print(f\"ðŸ”— View results in MLflow UI: {EVALUATION_EXPERIMENT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "703c59ed-ca70-4fce-8537-b736c0a5fb23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 11. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53f3a53e-092f-4aec-ae94-74aa94f61714",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {
        "data_path": "/databricks-datasets/learning-spark-v2/sf-airbnb/sf-airbnb-clean.parquet",
        "evaluation_experiment": "/Shared/mlops/model_evaluation",
        "evaluation_type": "comprehensive",
        "model_name": "workspace.default.california_housing_predictor",
        "model_version": "2"
       },
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create evaluation summary for next stage\n",
    "evaluation_summary = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'model_version': model_version_info.version,\n",
    "    'evaluation_run_id': evaluation_run_id,\n",
    "    'evaluation_decision': evaluation_decision,\n",
    "    'metrics': eval_metrics,\n",
    "    'experiment_name': EVALUATION_EXPERIMENT,\n",
    "    'evaluation_type': EVALUATION_TYPE,\n",
    "    'test_data_samples': len(X_test)\n",
    "}\n",
    "\n",
    "# Save to DBFS for next notebook\n",
    "import json\n",
    "dbutils.fs.put(\"/tmp/model_evaluation_results.json\", \n",
    "               json.dumps(evaluation_summary, indent=2, default=str))\n",
    "\n",
    "print(\"âœ… Evaluation results exported to /tmp/model_evaluation_results.json\")\n",
    "print(\"ðŸš€ Ready for approval workflow!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_model_evaluation (1)",
   "widgets": {
    "data_path": {
     "currentValue": "/databricks-datasets/learning-spark-v2/sf-airbnb/sf-airbnb-clean.parquet",
     "nuid": "0a6c6de9-33eb-4b56-80c8-2a6804c90bc3",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/databricks-datasets/learning-spark-v2/sf-airbnb/sf-airbnb-clean.parquet",
      "label": "Test Data Path",
      "name": "data_path",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "/databricks-datasets/learning-spark-v2/sf-airbnb/sf-airbnb-clean.parquet",
      "label": "Test Data Path",
      "name": "data_path",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "evaluation_experiment": {
     "currentValue": "/Shared/mlops/model_evaluation",
     "nuid": "731f60d1-9222-4f72-9447-6926ddc683ad",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/Shared/mlops/model_evaluation",
      "label": "Evaluation Experiment Path",
      "name": "evaluation_experiment",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "/Shared/mlops/model_evaluation",
      "label": "Evaluation Experiment Path",
      "name": "evaluation_experiment",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "evaluation_type": {
     "currentValue": "comprehensive",
     "nuid": "9460081e-e214-4781-90de-b0f3c950cdea",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "comprehensive",
      "label": "Evaluation Type",
      "name": "evaluation_type",
      "options": {
       "choices": [
        "quick",
        "comprehensive"
       ],
       "fixedDomain": true,
       "multiselect": false,
       "widgetDisplayType": "Dropdown"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "comprehensive",
      "label": "Evaluation Type",
      "name": "evaluation_type",
      "options": {
       "autoCreated": null,
       "choices": [
        "quick",
        "comprehensive"
       ],
       "widgetType": "dropdown"
      },
      "widgetType": "dropdown"
     }
    },
    "model_name": {
     "currentValue": "workspace.default.california_housing_predictor",
     "nuid": "f0ed75b3-5a47-4a22-b88f-d653f811d3ec",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "california_housing_predictor",
      "label": "Registered Model Name",
      "name": "model_name",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "california_housing_predictor",
      "label": "Registered Model Name",
      "name": "model_name",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "model_version": {
     "currentValue": "2",
     "nuid": "403ed953-a0b7-49a3-8cf7-c10762006cf3",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "latest",
      "label": "Model Version (or 'latest')",
      "name": "model_version",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "latest",
      "label": "Model Version (or 'latest')",
      "name": "model_version",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
